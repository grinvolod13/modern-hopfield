{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hopfield_modern.hopfield_modern import Hopfield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm, tqdm_notebook\n",
    "from typing import Callable\n",
    "import itertools as it\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(prediction, data):\n",
    "    t = prediction - data\n",
    "    # print(np.linalg.norm(t))\n",
    "    return 1 - np.linalg.norm(t)\n",
    "    # return np.sum(np.abs(data))\n",
    "    \n",
    "    \n",
    "def norm2(prediction, data):\n",
    "    t = (prediction - data)\n",
    "    return 1/(np.linalg.norm(t) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(data):\n",
    "    t = np.exp(data - np.max(data))\n",
    "    return t/np.sum(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dir(dataset: list, path: str, label: bool, sort=True):\n",
    "    dataset.sort(key=lambda e: e[\"id\"])\n",
    "    if dataset:\n",
    "        index = dataset[-1][\"id\"]+1\n",
    "    else:\n",
    "        index = 0\n",
    "    fnames = [(path+\"/Red/\"+i[:2]+\"_Red.txt\", path+\"/Green/\"+i[:2]+\"_Green.txt\",path+\"/Blue/\"+i[:2]+\"_Blue.txt\") for i in [i for i in os.walk(path)][1][2]]\n",
    "    for fname in fnames:\n",
    "        person = {\"id\": index}\n",
    "        person[\"label\"] = label\n",
    "        with open(fname[0],\"r\") as rfile, open(fname[1],\"r\") as gfile, open(fname[2],\"r\") as bfile:\n",
    "            person[\"r\"] = [float(i) for i in rfile.readlines()[1:]]\n",
    "            person[\"g\"] = [float(i) for i in gfile.readlines()[1:]]\n",
    "            person[\"b\"] = [float(i) for i in bfile.readlines()[1:]]\n",
    "        \n",
    "        if sort:\n",
    "            person[\"r\"].sort()\n",
    "            person[\"g\"].sort()            \n",
    "            person[\"b\"].sort()\n",
    "        \n",
    "        index+=1\n",
    "        dataset.append(person)\n",
    "        \n",
    "def load_dataset(dataset: list, path: str, pathpos: str, pathneg: str, sort=True):\n",
    "    load_dir(dataset, path+\"/\"+pathpos, True, sort=sort)\n",
    "    load_dir(dataset, path+\"/\"+pathneg, False, sort=sort)    \n",
    "        \n",
    "        \n",
    "# def get_filter(dataset: list, filter: str):\n",
    "#     if filter!=\"r\" and filter!=\"g\" and filter!=\"b\":\n",
    "#         raise ValueError('Wrong filter: must be \"r\", \"g\", \"b\"')\n",
    "#     new_dataset = []\n",
    "#     for person in dataset:\n",
    "#         new_dataset.append({\"id\": person[\"id\"], \"label\": person[\"label\"], \"data\": person[filter]})\n",
    "    \n",
    "#     return new_dataset\n",
    "\n",
    "def show_plot_by_filter(dataset: list, filter: str):\n",
    "    plt.clf()\n",
    "    if filter!=\"r\" and filter!=\"g\" and filter!=\"b\":\n",
    "        raise ValueError('Wrong filter: must be \"r\", \"g\", \"b\"')\n",
    "    for i in np.arange(0,len(dataset), 1):\n",
    "        color=\"green\"\n",
    "        if dataset[i][\"label\"]:\n",
    "            color=\"red\"\n",
    "        for j in dataset[i][filter]:\n",
    "            plt.plot(i,j, \"o\", markersize=0.5, color=color)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def binarize_person(person: dict, precision=1e-3, up=1.8, down=0.2, radius=0, flatten=True, filters=3):\n",
    "    person_data = np.zeros((3, int((up-down)//precision)+1), np.float32)[0:filters]\n",
    "    \n",
    "    dots  = (\n",
    "        ((np.array(person[\"r\"]).clip(down, up)-down)//precision).astype(np.uint32),\n",
    "        ((np.array(person[\"g\"]).clip(down, up)-down)//precision).astype(np.uint32),\n",
    "        ((np.array(person[\"b\"]).clip(down, up)-down)//precision).astype(np.uint32))[0:filters]\n",
    "\n",
    "    \n",
    "    for color_index in range(filters):\n",
    "        for dot in dots[color_index]:\n",
    "            person_data[color_index][max(0, dot-radius) : min(dot+radius+1, person_data.shape[1])].fill(1)\n",
    "    \n",
    "    if flatten:\n",
    "        return {\"id\": person[\"id\"], \"label\": person[\"label\"], \"data\": person_data.flatten()}\n",
    "    else:\n",
    "        return {\"id\": person[\"id\"], \"label\": person[\"label\"], \"data\": person_data}\n",
    "\n",
    "def binarize_person_by_filter(person: dict, precision=1e-3, up=1.8, down=0.2, radius=0, flatten=True, filter='g'):\n",
    "    person_data = np.zeros((int((up-down)//precision)+1), np.float32)\n",
    "    \n",
    "    dots  = ((np.array(person[filter]).clip(down, up)-down)//precision).astype(np.uint32)\n",
    "\n",
    "    \n",
    "    for dot in dots:\n",
    "        person_data[max(0, dot-radius) : min(dot+radius+1, person_data.shape[0])].fill(1)\n",
    "    \n",
    "    if flatten:\n",
    "        return {\"id\": person[\"id\"], \"label\": person[\"label\"], \"data\": person_data.flatten()}\n",
    "    else:\n",
    "        return {\"id\": person[\"id\"], \"label\": person[\"label\"], \"data\": person_data}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def show_plot_avg_by_filter(dataset: list, filter: str):\n",
    "    plt.clf()\n",
    "    if filter!=\"r\" and filter!=\"g\" and filter!=\"b\":\n",
    "        raise ValueError('Wrong filter: must be \"r\", \"g\", \"b\"')\n",
    "    for person in dataset:\n",
    "        \n",
    "        if person[\"label\"]:\n",
    "            color=\"red\"\n",
    "            x = 0\n",
    "        else:\n",
    "            color=\"green\"\n",
    "            x = 1\n",
    "             \n",
    "        for y in person[filter]:\n",
    "            plt.plot(x,y, \"o\", markersize=0.5, color=color)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_score_i(dataset: np.ndarray, X: np.ndarray, i: int, score_fn: Callable):\n",
    "    if i==0:\n",
    "        max_i = 1\n",
    "    else:\n",
    "        max_i = 0\n",
    "    # print(f\"len {len(dataset)}\")\n",
    "    for j in (k for k in range(len(dataset)) if k!=i):\n",
    "        if score_fn(X,dataset[max_i]) <= score_fn(X,dataset[j]):\n",
    "            max_i = j\n",
    "            \n",
    "    return max_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "dataset_full = []\n",
    "load_dataset(dataset_full, \"C:/Users/User/Desktop/Data/Data\", \"BC\", \"Control\", sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "N = 58 # const in test\n",
    "\n",
    "pos = random.sample(dataset_full[:68], N//2)\n",
    "neg = random.sample(dataset_full[68:], N//2)\n",
    "dataset = np.concatenate([pos, neg]).tolist()\n",
    "print(len(dataset))\n",
    "assert sum([i['label'] for i in dataset])*2 == N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d50847ebfc4f31a3da49b3240affc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_of_dims = 23 # const in test\n",
    "filter = 'b'\n",
    "\n",
    "result = []\n",
    "\n",
    "\n",
    "image_scaling_test = np.linspace(10, 60, 10, dtype=int)\n",
    "tests_per_generation = 1000\n",
    "\n",
    "\n",
    "\n",
    "for image_scaling in tqdm_notebook(image_scaling_test):\n",
    "    one_scale = []\n",
    "    \n",
    "    \n",
    "    for test in range(tests_per_generation):\n",
    "        one_test = []\n",
    "        \n",
    "        dataset_dim = np.array([{'data': random.sample(person[filter], num_of_dims),\n",
    "                                'label': person['label'],\n",
    "                                'id': person['id']\n",
    "                                } for person in dataset])\n",
    "\n",
    "        dataset_color = np.array([i['data'] for i in dataset_dim])\n",
    "        dataset_color = np.array([np.concatenate([i]*image_scaling) for i in dataset_color])\n",
    "\n",
    "\n",
    "        for p in dataset_color:\n",
    "            p.sort()\n",
    "            \n",
    "        dataset_color -= dataset_color.min()\n",
    "        dataset_color /= dataset_color.max()\n",
    "        # dataset_color = softmax(dataset_color) # not!!!\n",
    "\n",
    "        dataset_color = dataset_color*2-1\n",
    "\n",
    "        for i in range(len(dataset_color)):\n",
    "            \n",
    "            _mask = np.ones(len(dataset_color), dtype=bool)\n",
    "            _mask[i] = False\n",
    "\n",
    "            _X = dataset_color[i]\n",
    "            _images = dataset_color[_mask]\n",
    "            \n",
    "            model = Hopfield(_images.T.copy())\n",
    "            out = model.run(_X, 16.0)\n",
    "            max_i = max_score_i(dataset=dataset_color, X=out, i=i, score_fn=norm)\n",
    "            \n",
    "            \n",
    "            if dataset_dim[max_i][\"label\"]==dataset_dim[i][\"label\"]:\n",
    "                one_test.append(1)\n",
    "            else:\n",
    "                one_test.append(0)\n",
    "    \n",
    "        one_scale.append(np.average(one_test))\n",
    "    result.append((image_scaling, dataset_color.shape[0]/dataset_color.shape[1], np.average(one_scale)))\n",
    "\n",
    "\n",
    "pd.DataFrame(result).to_csv(\"results.csv\", index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
